
\section{Introduction}
As transistors continue to decrease in size, as they have been for the last fifty years, more transistors are able to fit into a single processor.
Historically this has led to faster processors, allowing programs to take advantage of the increased speed with no effort from the programmer.
Recently, however, this increase in processor speed has come to a halt.
Being unable to increase processor speed, manufacturers have instead started focussing the new transistors on multi-core processors.

The ubiquity of multi-core processors means that sequential programs can no longer take full advantage of the processor.
However, writing parallel programs is significantly more difficult than writing sequential programs.
Nested data parallelism\cite{blelloch1995nesl} lifts the burden of exploiting multi-cores from the programmer to the compiler-writer.
Current implementations of nested data parallelism use impressive optimisation techniques such as fusion\cite{lippmeier2013flow}, but are still unable to compete with handwritten parallel code.

There have historically been two major problems with nested data parallelism:
both are a result of the flattening transform changing asymptotic complexity of the program.
The problem of increased work complexity has been solved by using virtual replication\cite{lippmeier2012work}.
The space complexity problem is when a nested data parallel program requires
significantly more memory than the original program\cite{spoonhower2008space}.
It is possible that our recent flow fusion work\cite{lippmeier2013flow}
is able to solve some of the space complexity cases,
but more thorough investigation is required to classify the problem.

Purely functional languages, such as Haskell, are more restricted than impure imperative languages
in the sense that arbitrary expressions may not perform side-effects such as destructive updates.
This restriction, however, turns out to have serious benefits:
an optimising compiler is able to reorder expressions to gain better performance,
without any risk of changing the meaning of the program.
From an engineering perspective also, the separation of side-effects from pure computation leads to clearer, easier to understand and verify code.

Improvement theory\cite{sands1998improvement} is a method of verifying that particular
program transformations are, in fact, optimisations.
The requirements for an optimisation is that it must not change the meaning of the program
and must also be some kind of improvement, either in terms of space or runtime.
Improvement theory has been extended to work with lazy languages such as Haskell\cite{moran1999improvement}.

The overall goal of this research is to improve the runtime and space performance of Data Parallel Haskell.
While much can be done to improve the base library, the benefits are generally only constant factor improvements.
Better compiler optimisations are always helpful, but one of the problems at the moment is the opacity of compiler transforms.
This opacity means that, given an input program,
a programmer does not know whether the resulting optimised program will be optimal.
In order to judge the quality of optimisation, the programmer must inspect the resulting \emph{core code} and guess about its efficiency, and what can be done to improve it.

