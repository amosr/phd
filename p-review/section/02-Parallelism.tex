\section{Parallelism}
Writing parallel programs that operate on a large amount of data can be difficult.
To achieve the best performance on a multi-processor machine, programs must be written using low-level synchronisation techniques such as threads and locking.
These methods are error prone, and improper use can lead to bugs, such as deadlocks and \emph{heisenbugs}.
In contrast to bugs in a sequential program, these bugs are often much harder to reproduce because of the delicate timing and communication between threads.
So-called heisenbugs are a particularly hard class of bug, as they disappear and the program appears to work only when debugging information is added.

There are many techniques for making parallelism easier to take advantage of,
each with their own advantages and disadvantages.
In general, the more restrictive a technique is, the easier it is to achieve adequate performance
as there is more knowledge about the program's structure.
However, these restrictions come at the expense of making it harder to express some programs.
We discuss three techniques:

\begin{description}
\item[Flat   data parallelism]
is able to achieve speeds comparable to C with advanced fusion techniques,
but only supports regular structures, such as rectangular and cubic arrays.

\item[Nested data parallelism]
is similar to flat data parallelism,
but supports parallel operations over tree-like structures
while keeping the load balanced over processors.

\item[Feedback directed automatic parallelism]
is able to automatically parallelise any program,
using profiling information from running the program
to find the best places to introduce parallelism. 
\end{description}


\subsection{Flat data parallelism}
Flat data parallelism is a restricted form of data parallelism that disallows nested parallelism:
only the top-most computation is performed in parallel.
To ensure a balanced load across parallel computations, only regular structures such as rectangular arrays can be used.
Programs are expressed using combinators such as @map@, @filter@, @sum@ and @fold@.

Accelerate is an \emph{embedded domain-specific language} in Haskell, specifically for running parallel computations on graphics processors.
As a restricted domain-specific language, it targets graphics processors\cite{chakravarty2011accelerating} and uses fusion to remove intermediate arrays.
With these, and other optimisations, it is sometimes able to achieve speeds similar to hand-optimised CUDA code\cite{mcdonell2013optimising}.

Repa is a Haskell library with combinator support for regular parallel array computations\cite{keller2010regular}.
Type indices are used to distinguish between so-called \emph{delayed arrays} and \emph{manifest arrays}.
Delayed arrays will not be reified in memory at runtime, but are instead expressed as functions of the array index.
With sufficient inlining and other general purpose optimisations, delayed arrays will be removed and fused together\cite{lippmeier2012guiding}.
Efficient stencil convolution is supported as a special case.
To remove branching from the worker loops, the edges of a stencil are computed in different loops\cite{lippmeier2011efficient}.

Many algorithms are more naturally expressed using nested parallelism, such as the hierarchical N-body calculation\cite{barnes1986hierarchical} algorithm and sparse-matrix vector multiplication.
For these algorithms, flat data parallelism is insufficient.


\subsection{Nested data parallelism}
Nested data parallelism, as introduced in a seminal paper\cite{blelloch1990vector},
uses a transform known as \emph{flattening} to convert nested data parallelism into flat data parallelism
while maintaining a balanced load across processors.

A language specifically for nested data parallelism, \nesl\cite{blelloch1995nesl},
uses the flattening transform to remove nested parallelism.
It targets a virtual machine called \vcode\cite{blelloch1993implementation}
which is then compiled to C to run on vector machines such as CRAY.
Many example programs have been implemented in \nesl\cite{blelloch1996programming}
but it is not a general purpose language and does not support higher-order functions\cite{leshchinskiy2005higher} or recursive structures \cite{keller1998flattening}.

Attempts have been made to introduce nested data parallelism to imperative languages such as Fortran\cite{au1997enlarging}.
These have the benefit of taking advantage of existing compiler optimisations.
The problem with targeting an impure language, however,
is that arbitrary expressions may perform side-effects.
This severely limits the places where parallelism may be introduced, as performing unknown side-effects in parallel will likely change the meaning of the program.

\subsubsection{Data Parallel Haskell}
Data Parallel Haskell (DPH) is an implementation of nested data parallelism for Haskell\cite{chakravarty2007data}.
The original flattening transform is extended to support higher-order functions\cite{leshchinskiy2006higher}, a common pattern in Haskell,
and arbitrary Haskell types: sum, product and recursive data types\cite{chakravarty2000more}.

Instead of targeting vector machines as \nesl~does, DPH targets commodity symmetric multi-processor (SMP) machines.
Fusion is performed to remove intermediate arrays, but care must be taken not to reduce parallelism\cite{chakravarty1999portable}.
This is solved by partitioning the program into local operations, requiring only some small chunk of the array,
and global operations, requiring thread synchronisation\cite{keller1999distributed}.
As long as only local operations are fused, crossing no global boundaries, no parallelism will be lost.

Unlike \nesl, DPH does not require the entire program to be written using nested data parallelism,
and supports partial vectorisation\cite{chakravarty2008partial}.
This allows other parts of the program to be expressed naturally as sequential computations, such as drawing to the screen or reading data from disk,
while the core computation can be run in parallel.

The original flattening transform also increases the time and space complexity of the program,
as flattening causes data to be replicated multiple times,
and if fusion does not occur correctly, such intermediate structures will be reified in memory.
The time complexity problem can be fixed by using a clever representation, making replication constant time\cite{lippmeier2012work}.
This does not solve the memory problem, however.

As with all optimisations, conversion from nested data parallelism to flat data parallelism should be semantics preserving:
the original program's meaning must remain unchanged\cite{leshchinskiy2005higher}.
This is complicated by the fact that Haskell is a lazy language, whereas, for efficiency, strict unboxed arrays are used to represent data.
Several tricks are used to preserve laziness while still using strict unboxed arrays.

Flattening converts scalar operations into array operations, even for intermediate scalars\cite{keller2012vectorisation}.
This means that operations that once were simple additions on registers must now work on arrays in memory.
Array fusion can often remove these intermediate structures, but fusion relies heavily on other optimisations such as inlining,
which may not always occur due to sharing. Excessive inlining can also cause code blowup in the compilation process,
leading to long compile times.
Instead of vectorising an entire program and then relying on fusion to remove intermediate arrays,
identifying scalar operations and not vectorising them has proven to be less fragile.




%\rev{Scans as Primitive Parallel Operations}{blelloch1989scans}
%If distributed vector machines implemented scan operations on vectors, much time could be saved that would otherwise be spent waiting for memory accesses to return.
%Explanation of radix sort example.
%Segmented scan operations use a vector of flags the same length as the values. I guess this totally disallows empty segments, which makes sense. True flag indicates the start of a segment. What happens if there is no True flag at the start..?
%Clever implementation of Quicksort using segmented scans and iteratively splitting each segment into more subsegments. Using the segment flag representation allows very easy splitting.
%It seems quite disingenuous to claim ``constant time for a vector of size n, provided there are n nodes in the machine''. It does deal with this eventually though, in the load-balancing section.
%Hardware description of a tree scan.

% \rev{A Comparison of Sorting Algorithms for the Connection Machine CM-2}{blelloch1991comparison}
% Different sorting algorithms are implemented and compared for the connection machine. Irrelevant.
% \rev{Nepal - Nested Data-Parallelism in Haskell}{chakravarty2001nepal}
% \rev{Harnessing the Multicores: Nested Data Parallelism in Haskell}{jones2008harnessing}
% \rev{Costing nested array codes}{lechtchinsky2002costing}





\subsection{Feedback directed automatic parallelism}

Automatic parallelism, as in Mercury\cite{bone2010automatic},
is able to parallelise more general programs than nested data parallelism,
but at the cost of compiler simplicity.

Feedback directed parallelism relies on the program being run multiple times,
and each time the program is recompiled.
The compiler uses information from the previous runs to determine the best place to introduce parallelism.

Mercury, a logic language, has many similarities to Haskell: it is pure, statically typed, and has type inference.
The purity allows the compiler to know where it is safe to introduce parallelism.

Automatic parallelism is a far more challenging problem than nested data parallelism,
since the programs do not use any known set of combinators or structure.
For this reason, and because many important scientific programs can be implemented with nested data parallelism\cite{blelloch1996programming}, 
automatic parallelism will not be discussed further.

