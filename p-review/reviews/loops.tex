\rev{Collective loop fusion for array contraction}{gao1993collective}
Array contraction transforms array variables into scalar variables,
potentially allowing them to be stored in registers.
Loops are fused in order to maximise array contraction opportunities.

Reads and writes to arrays are far more costly than scalar variables, since
scalars can be stored in registers but arrays must go through the memory system.
Even if the requested memory is in cache, a cache hit is still far more expensive than a register read.

A \emph{loop dependence graph} (LDG) is created for a single-entry, single-exit region with $k$ ``identically control dependent'' perfect loop nests with equivalent index spaces.
A loop nest is a set of nested loops; a perfect loop nest only has statements inside the innermost loop.
There is a node in the LDG for each loop nest.
The edges are loop-independent data dependencies, which are classified as \emph{fusible} or \emph{nonfusible}.
Fusing \emph{nonfusible} edges would violate data dependence. \emph{Fusible} edges can be \emph{contractable} or \emph{noncontractable}.
Fusing a \emph{contractable} edge will save memory access if data is already available in registers.
This can be generalised to allow distinct index spaces by marking any dependencies between difference spaces nonfusible. A single statement is a degenerate case (loop with unit index space).

First, the nodes are partitioned into fusible clusters. Then some kind of maxflow/mincut algorithm is applied...


\rev{Better tiling and array contraction for compiling scientific programs}{pike2002better}
Titanium, a dialect of Java with {\tt foreach} loops that execute in unspecified order over some subset of $\mathbb{Z}^N$.
Instead of static analysis of loop bounds, array bounds and array aliasing, cheap runtime tests are used
and both optimised and unoptimised versions are generated.
Instead of modelling register reuse or cache behaviour, the compiler measures program runtime with different settings and chooses the best.
Tile space $T$ is defined at $\mathbb{Z}^N \times {0, ... , K - 1}$ where the tile size is $K$.

The array contraction here is more to do with replacing arrays with smaller temporary buffer arrays, than scalar variables.
The tiling is chosen so that the buffers can be small.

\rev{Locality Optimizations for Multi-Level Caches}{rivera1999locality}
It turns out that most cache-aware optimisations can benefit from multi-level caches just by targeting L1.

\rev{Optimization of Array Accesses by Collective Loop Transformations}{sarkar1991optimization}
Array contraction is presented: replacing an array variable with a scalar variable or a buffer with a few scalar variables.
Loops are analysed; when a loop produces an array in the same order as it is consumed in another loop, there is an opportunity to
remove the intermediate array.

Access vectors of arrays are determined by the array subscripts inside the loop.
A \emph{loop communication graph} is created from a set of loop nests. A node is created for each loop nest, with input ports and output ports for each input and output array of the loop. Each port is annotated with the array's access vector.
Edges represent array communication between an output port of the source loop nest and the input port of the destination loop nest.
An edge is \emph{consistent} if both ports have the same array access vector, meaning they access the array in the same way.

The goal, then, is to minimise the number of inconsistent ports to allow more opportunities for array contraction.
\begin{itemize}
\item
\emph{Loop reversal} flips an array's access vector and all the subscripts inside the loop, by reversing the index space.
\end{itemize}
For one dimension, the problem of choosing which loops to reverse is mapped to a 2-colouring problem.
A new graph is created with a positive and negative for each node in the LCG. For each edge, the positive of the input is connected to the negative output if the edge is consistent, otherwise the positive output.
This means that two nodes connected by an inconsistent edge cannot have the same colour in their positive node.

The graph is then coloured and for each positive node, if it is colour one it is left alone, if it is colour two it is reversed.
If the graph cannot be coloured an edge is removed and colouring is again attempted.
Weightings are added to the edges to prioritise removal of edges and to never remove the edges between positive and negative of one node.

For multi-dimensional loops, a loop nest may be interchanged, thus swapping the array access vector.

\rev{Improving Data Locality by Array Contraction}{song2004improving}

\rev{Loop scheduling with memory access reduction subject to register constraints for DSP applications}{wang2013loop}
Quite low-level, working on assembly language.
Reduce memory accesses in cases where a specific array element is used in successive iterations.
  for (int i =...)
    k[i] = k[i] * k[i+1];
The memory access for k[i+1] should be stored in a register and kept for the next iteration's k[i].
Build a DAG out of loop body (with no loop ??), then create a memory access graph (MAG) to describe the memory dependence of operations over successive iterations.
After the MAG is created, a register usage scheduling is created and used to decide which registers to use.


\rev{Loop Transformations for Restructuring Compilers}{banerjee1993loop}
The first part focuses on mathematical background: solving linear algebra integer equations, and integer and unimodular matrices.
Unimodular matrices are integral, and their inverse is also integral and unimodular. GCD can be found by solving a linear equation.

A sequential program has a total order for its execution. In order to make the most of the hardware, a restructuring compiler
will try to find a new execution order. The new execution order must not change the meaning of the program.
Th meaning is preserved by obeying the constraints of the `dependence structure' of the program:
the way memory locations are read from and written to.
Control flow in conditionals is ignored in this book, as they only treat loop nests of assignment statements.

Types of dependencies for $S \delta T$ ($T$ depends on $S$):
\begin{itemize}
\item \emph{Flow dependent}: $S$ writes $M$, $T$ reads $M$
\item \emph{Anti-dependent}: $S$ reads $M$, $T$ writes
\item \emph{Output dependent}: $S$ and $T$ both write
\item \emph{Input dependent}: $S$ and $T$ both read
\end{itemize}

Input dependences can be ignored for the actual meat.

\emph{Iteration-level} loop transformations execute each iteration of the loop in the same order, but can change the order in which iterations are performed.
To be valid, each iteration $H(j)$ that depends on an iteration $H(i)$ must still be executed in that order.
\emph{Iteration graph partitioning} finds the weakly connected components of the iteration dependence graph and executes them in parallel. For each component, its iterations are then performed sequentially.
For example,
\begin{code}
do I = 0, 100
    X(I) = ...
    ...  = X(I - 4) + X(I - 6)
enddo
\end{code}
In this case there are two dependences: $H(i)$ depends on $H(i-4)$ and $H(i-6)$.
The \emph{greatest common divisor} of 4 and 6 is 2, so this can be split into two components: odd and even.
These components can be executed in parallel.

\emph{Unimodular transforms}
